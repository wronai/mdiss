"""AI-powered ticket generation for issue trackers using local LLM (Mistral 7B via Ollama)."""
import json
import logging
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import ollama
import requests
from dotenv import load_dotenv

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AITicketGenerator:
    """Generate tickets using local LLM models via Ollama."""

    def __init__(
        self,
        model: str = "mistral:latest",
        temperature: float = 0.7,
        max_tokens: int = 1000,
        ollama_host: str = "http://localhost:11434",
    ):
        """Initialize the AI ticket generator with local LLM.

        Args:
            model: Name of the Ollama model to use (default: mistral:latest)
            temperature: Controls randomness (0.0 to 1.0)
            max_tokens: Maximum number of tokens to generate
            ollama_host: Base URL for Ollama API
        """
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.ollama_host = ollama_host.rstrip("/")

        # Test the connection to Ollama
        self._verify_ollama_connection()

    def generate_enhanced_description(self, prompt: str, timeout: int = 120) -> str:
        """Generate an enhanced description using the local LLM.

        Args:
            prompt: The prompt to send to the LLM
            timeout: Maximum time in seconds to wait for the response

        Returns:
            str: Enhanced description generated by the LLM or an error message
        """
        try:
            # First, verify the model is available
            available_models = [m["name"] for m in ollama.list().get("models", [])]
            if self.model not in available_models:
                error_msg = f"Model {self.model} not found. Available models: {', '.join(available_models)}"
                logger.error(error_msg)
                return f"Error: {error_msg}"

            logger.info(f"Generating enhanced description using model: {self.model}")

            # Generate the response with timeout
            response = ollama.generate(
                model=self.model,
                prompt=prompt,
                options={
                    "temperature": min(
                        max(0.1, self.temperature), 1.0
                    ),  # Ensure valid range
                    "num_predict": min(
                        max(100, self.max_tokens), 4000
                    ),  # Reasonable limits
                    "timeout": timeout * 1000,  # Convert to milliseconds
                },
                stream=False,  # We want a complete response
            )

            if not response or "response" not in response:
                error_msg = "Unexpected response format from Ollama API"
                logger.error(f"{error_msg}: {response}")
                return f"Error: {error_msg}. Check server logs for details."

            return response["response"].strip()

        except requests.exceptions.Timeout:
            error_msg = f"Request to Ollama API timed out after {timeout} seconds"
            logger.error(error_msg)
            return f"Error: {error_msg}. The model might be too slow or unresponsive."

        except requests.exceptions.ConnectionError as e:
            error_msg = (
                f"Cannot connect to Ollama server at {self.ollama_host}. Is it running?"
            )
            logger.error(f"{error_msg} {str(e)}")
            return f"Error: {error_msg} Run 'make llm-serve' to start it."

        except Exception as e:
            error_msg = f"Error generating enhanced description: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return f"Error: {error_msg}. Check the logs for more details."

    def _verify_ollama_connection(self):
        """Verify that Ollama is running and the model is available."""
        try:
            # Check if Ollama is running
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=10)
            response.raise_for_status()

            # Check if the model is available
            models = [m["name"] for m in response.json().get("models", [])]
            if self.model not in models:
                logger.warning(
                    f"Model {self.model} not found in Ollama. Available models: {', '.join(models)}"
                )
                logger.info(f"Pulling model {self.model}...")
                ollama.pull(self.model)

        except requests.exceptions.RequestException as e:
            raise ConnectionError(
                f"Failed to connect to Ollama at {self.ollama_host}. "
                "Please make sure Ollama is running and the host is correct."
            ) from e

    def generate_ticket(
        self,
        title: str,
        description: str,
        context: Optional[Dict[str, Any]] = None,
        template: Optional[Union[str, Path]] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """Generate a ticket using AI.

        Args:
            title: Brief title of the ticket
            description: Detailed description of the issue
            context: Additional context for the AI
            template: Path to a template file or template string
            **kwargs: Additional parameters for the AI model

        Returns:
            Dictionary with generated ticket fields
        """
        # Prepare the prompt
        system_prompt, user_prompt = self._prepare_prompts(
            title, description, context, template
        )

        # Call the local LLM
        response = self._call_ollama(system_prompt, user_prompt, **kwargs)

        # Parse the response
        try:
            ticket_data = json.loads(response)
            if not isinstance(ticket_data, dict):
                ticket_data = {"content": response}
        except json.JSONDecodeError:
            ticket_data = {"content": response}

        return ticket_data

    def _prepare_prompts(
        self,
        title: str,
        description: str,
        context: Optional[Dict[str, Any]] = None,
        template: Optional[Union[str, Path]] = None,
    ) -> tuple[str, str]:
        """Prepare system and user prompts for the local LLM."""
        # System prompt with clear instructions for JSON output
        system_prompt = """You are a helpful assistant that generates well-structured tickets
for issue tracking systems like GitHub and GitLab. Your responses must be in valid JSON format
with appropriate fields for the ticket. Only respond with the JSON object, no additional text.

Example response format:
{
  "title": "Brief title of the issue",
  "description": "Detailed description of the issue with markdown formatting",
  "labels": ["bug", "priority-high"],
  "assignees": ["username1"],
  "priority": "high",
  "type": "bug"
}"""

        # Load template if provided
        template_text = ""
        if template:
            if isinstance(template, (str, Path)) and Path(template).exists():
                with open(template, "r", encoding="utf-8") as f:
                    template_text = f.read()
            else:
                template_text = str(template)

        # Build the user prompt with all context
        user_prompt_parts = [
            f"Create a well-structured issue ticket with the following details:",
            f"Title: {title}",
            f"Description:\n{description}",
        ]

        if template_text:
            user_prompt_parts.append(
                f"\nUse this template as a guide:\n{template_text}"
            )

        if context:
            context_str = "\n".join(f"- {k}: {v}" for k, v in context.items())
            user_prompt_parts.append(f"\nAdditional context:\n{context_str}")

        user_prompt = "\n".join(user_prompt_parts)
        user_prompt += (
            "\n\nRespond with only a valid JSON object containing the ticket details."
        )

        return system_prompt, user_prompt

    def _call_ollama(self, system_prompt: str, user_prompt: str, **kwargs) -> str:
        """Call the local Ollama API with the given prompts."""
        try:
            # Prepare the messages for the chat API
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]

            # Make the request to Ollama
            response = ollama.chat(
                model=self.model,
                messages=messages,
                options={
                    "temperature": kwargs.get("temperature", self.temperature),
                    "num_predict": kwargs.get("max_tokens", self.max_tokens),
                },
            )

            return response["message"]["content"]

        except Exception as e:
            logger.error(f"Error calling Ollama API: {str(e)}")
            raise RuntimeError(f"Failed to generate response from local LLM: {str(e)}")


def generate_github_issue(
    title: str,
    description: str,
    labels: Optional[List[str]] = None,
    assignees: Optional[List[str]] = None,
    **kwargs,
) -> Dict[str, Any]:
    """Generate a GitHub issue using AI.

    Args:
        title: Issue title
        description: Issue description
        labels: List of labels
        assignees: List of usernames to assign
        **kwargs: Additional parameters for the AI model

    Returns:
        Dictionary with generated issue fields
    """
    generator = AITicketGenerator(**kwargs)
    context = {
        "platform": "GitHub",
        "labels": ", ".join(labels) if labels else "",
        "assignees": ", ".join(assignees) if assignees else "",
    }
    return generator.generate_ticket(title, description, context)


def generate_gitlab_issue(
    title: str,
    description: str,
    labels: Optional[List[str]] = None,
    assignee_ids: Optional[List[int]] = None,
    **kwargs,
) -> Dict[str, Any]:
    """Generate a GitLab issue using AI.

    Args:
        title: Issue title
        description: Issue description
        labels: List of labels
        assignee_ids: List of user IDs to assign
        **kwargs: Additional parameters for the AI model

    Returns:
        Dictionary with generated issue fields
    """
    generator = AITicketGenerator(**kwargs)
    context = {
        "platform": "GitLab",
        "labels": ", ".join(labels) if labels else "",
        "assignee_ids": ", ".join(map(str, assignee_ids)) if assignee_ids else "",
    }
    return generator.generate_ticket(title, description, context)
